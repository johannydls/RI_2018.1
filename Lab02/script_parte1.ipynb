{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperação da Informação e Busca na Web - 2018.1\n",
    "\n",
    "### Atividade: Lab 02 - Parte 1 - Expansão de Consultas\n",
    "### Aluno: Johanny de Lucena Santos\n",
    "\n",
    "## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações necessárias para a análise dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "from scipy import sparse\n",
    "from nltk import bigrams\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo download de biblioteca necessária para realizar a tokenização das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Definindo o arquivo com os dados a ser analisado e o gabarito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/estadao_noticias_eleicao.csv', sep=',', encoding=\"utf-8\")\n",
    "\n",
    "#Substituindo linhas com valores NaN por string vazia, para não atrapalhar nas operações\n",
    "dataset = dataset.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento do Gabarito\n",
    " Usando a biblioteca **Abstract Syntax Trees** (ast) para converter as strings do gabarito para o objeto lista, junto com a função apply() da biblioteca **Pandas** para aplicar a conversão em todas as linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gabarito = pd.read_csv('../data/gabarito.csv')\n",
    "\n",
    "gabarito['google']        = gabarito['google'].apply(ast.literal_eval)\n",
    "gabarito['busca_binaria'] = gabarito['busca_binaria'].apply(ast.literal_eval)\n",
    "gabarito['tf']            = gabarito['tf'].apply(ast.literal_eval)\n",
    "gabarito['tfidf']         = gabarito['tfidf'].apply(ast.literal_eval)\n",
    "gabarito['bm25']          = gabarito['bm25'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções para avaliação das saídas utilizando a métrica Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para remover acentuação das palavras dos documentos\n",
    "Módulo **re** e **unicodedata** necessários para tratamento de expressões regulares e normalização de string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_acentuacao(palavra):\n",
    "    pattern = re.compile('[^a-zA-Z0-9 ]')\n",
    "    palavra = normalize('NFKD', palavra).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    return pattern.sub(' ', palavra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando junção dos títulos, subtítulos e conteúdos\n",
    "Na junção, está sendo removidos todos os acentos para facilitar a criação dos tokens dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documentos = dataset.titulo + \" \" + dataset.subTitulo + \" \" + dataset.conteudo\n",
    "documentos = documentos.apply(lambda palavra: remove_acentuacao(palavra).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix and Vocabulary construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação dos docIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docIDs = dataset.idNoticia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os tokens dos dados e a frequência de termos de cada documento da lista de postings\n",
    "Utilizando a função **Count** da biblioteca **Collections** para realizar a contagem dos termos (frequência)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = documentos.apply(nltk.word_tokenize)\n",
    "tokens_list = documentos.apply(lambda text: text.lower().split())\n",
    "tokens2 = [token for tokens_list in tokens_list for token in tokens_list]\n",
    "termo_frequency = tokens.apply(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocab = co_occurrence_matrix(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consult bigram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultable_matrix = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def consult_frequency(w1, w2):\n",
    "    return (consultable_matrix[vocab[w1], vocab[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_3_words(termo):\n",
    "    ranking = []\n",
    "    for termo_matrix in consultable_matrix:\n",
    "        ranking.append(consult_frequency(termo, termo_matrix))\n",
    "    \n",
    "    return sorted(ranking, reverse=True)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25981\n",
      "11196\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(vocab['poucos'])\n",
    "print(vocab['recursos'])\n",
    "print(type(consultable_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função geradora do índice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indiceInvertido = {}\n",
    "\n",
    "def gerarIndiceInvertido():\n",
    "    for i in range(len(tokens)):\n",
    "        idNoticia = docIDs[i]\n",
    "        palavras = tokens[i]\n",
    "    \n",
    "        for palavra in palavras:\n",
    "            if palavra not in indiceInvertido:\n",
    "                indiceInvertido[palavra.lower()] = {}\n",
    "        \n",
    "            if not indiceInvertido[palavra.lower()].get(idNoticia):\n",
    "                docs = indiceInvertido[palavra.lower()]\n",
    "                docs[idNoticia] = termo_frequency[i][palavra.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gerarIndiceInvertido()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para retornar as palavras mais proximas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_aproximada(termo, qtd):\n",
    "    termos = indiceInvertido.keys()\n",
    "    return sorted(termos, key=lambda palavra: nltk.edit_distance(termo.lower(),palavra))[0:qtd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para gerar um dicionário com os pesos dos índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gerar_dict_pesos(frase, gerador_peso):\n",
    "    termos = frase.split(\" \")\n",
    "    docs_peso = {}\n",
    "    \n",
    "    for i in range(len(termos)):\n",
    "        termo = termos[i].lower()\n",
    "        docs = indiceInvertido[termo]\n",
    "        \n",
    "        for doc_id in docs:\n",
    "            tf = docs[doc_id]\n",
    "            \n",
    "            if doc_id not in docs_peso:\n",
    "                docs_peso[doc_id] = np.array([0 if j != i else gerador_peso(tf) for j in range(len(termos))])\n",
    "            else:\n",
    "                doc_vector = docs_peso[doc_id]\n",
    "                docs_peso[doc_id] = np.array([doc_vector[j] if j != i else gerador_peso(tf) for j in range(len(termos))])\n",
    "    \n",
    "    return docs_peso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para gerar um dicionário com vetores binários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gerar_binario(frase):\n",
    "    def gerador_peso(tf):\n",
    "        return 1\n",
    "    return gerar_dict_pesos(frase, gerador_peso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para gerar o TF da consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gerar_vetor_tf(frase):\n",
    "    def gerador_peso(tf):\n",
    "        return tf\n",
    "    return gerar_dict_pesos(frase, gerador_peso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para gerar IDF dos termos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gerar_vetor_tfidf(frase):\n",
    "    termos = frase.split(\" \")\n",
    "    vetor_tfidf = np.array([math.log((len(documentos)+1)/len(indiceInvertido[termo.lower()])) for termo in termos])\n",
    "    return vetor_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para gerar o vetor binário de consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gerar_consulta(frase):\n",
    "    termos = frase.split(\" \")\n",
    "    vetor = np.array([1 if indiceInvertido.get(termo.lower()) else 0 for termo in termos])\n",
    "    return vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para gerar um dicionário com vetores BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gerar_vetor_bm25(frase):\n",
    "    docs_tf = gerar_vetor_tf(frase)\n",
    "    k = 5\n",
    "    vetor_bm25 = {doc_id: np.array([((k+1)*tf)/(tf+k) for tf in vetor_tf]) for doc_id, vetor_tf in docs_tf.items()}\n",
    "    \n",
    "    return vetor_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de busca binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_binaria(frase):\n",
    "    docs_tf = gerar_binario(frase)\n",
    "    consulta = gerar_consulta(frase)\n",
    "    doc_rank = sorted(list(docs_tf.items()), key=lambda doc: np.dot(doc[1], consulta), reverse=True)[:5] \n",
    "    return [doc[0] for doc in doc_rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de busca por TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_tf(frase):\n",
    "    docs_tf = gerar_vetor_tf(frase)\n",
    "    query = gerar_consulta(frase)\n",
    "    doc_rank = sorted(list(docs_tf.items()), key=lambda doc: np.dot(doc[1], query), reverse=True)[:5] \n",
    "    return [doc[0] for doc in doc_rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de busca por TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_tf_idf(frase):\n",
    "    doc_tf = gerar_vetor_tf(frase)\n",
    "    doc_idf = gerar_vetor_tfidf(frase)\n",
    "    doc_rank = sorted(list(doc_tf.items()), key=lambda doc: np.dot(doc[1], doc_idf), reverse=True)[:5]\n",
    "    return [doc[0] for doc in doc_rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de busca por BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_bm25(frase):\n",
    "    doc_bm25 = gerar_vetor_bm25(frase)\n",
    "    consulta = gerar_consulta(frase)\n",
    "    doc_rank = sorted(list(doc_bm25.items()), key=lambda doc: np.dot(doc[1], consulta), reverse=True)[:5]\n",
    "    return [doc[0] for doc in doc_rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo consult frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 'poucos'\n",
    "w2 = 'recursos'\n",
    "consult_frequency(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-0e1225080384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_3_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'poucos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-99d2b48e69b8>\u001b[0m in \u001b[0;36mtop_3_words\u001b[0;34m(termo)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtermo_matrix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsultable_matrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsult_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermo_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-2931d05c9ff8>\u001b[0m in \u001b[0;36mconsult_frequency\u001b[0;34m(w1, w2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconsult_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconsultable_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "top_3_words('poucos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 693)\t16\n",
      "  (0, 2631)\t5\n",
      "  (0, 2752)\t2\n",
      "  (0, 3118)\t4\n",
      "  (0, 4719)\t1\n",
      "  (0, 5836)\t4\n",
      "  (0, 5850)\t9\n",
      "  (0, 7106)\t1\n",
      "  (0, 7984)\t1\n",
      "  (0, 12977)\t1\n",
      "  (0, 13866)\t1\n",
      "  (0, 15175)\t1\n",
      "  (0, 15418)\t2\n",
      "  (0, 19078)\t1\n",
      "  (0, 19655)\t1\n",
      "  (0, 22422)\t1\n",
      "  (0, 25894)\t1\n",
      "  (0, 27039)\t1\n",
      "  (0, 29645)\t3\n",
      "  (0, 31293)\t1\n",
      "  (0, 31321)\t1\n",
      "  (0, 32247)\t1\n",
      "  (0, 33379)\t6\n",
      "  (0, 35181)\t1\n",
      "  (0, 35402)\t2\n",
      "  (0, 40463)\t1\n",
      "  (0, 42548)\t3\n",
      "  (0, 45814)\t1\n",
      "  (0, 47352)\t1\n",
      "  (0, 50246)\t1\n",
      "  (0, 50771)\t1\n"
     ]
    }
   ],
   "source": [
    "tst = []\n",
    "for term in consultable_matrix:\n",
    "    tst.append(term)\n",
    "\n",
    "print(tst[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
